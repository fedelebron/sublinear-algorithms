\documentclass[11pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\usepackage[german,spanish,english]{babel}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\title{Sublinear algorithms exam}
\author{Fernandez Abrevaya, Lebrón, Vassiliev}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section*{Problem 1}
\addtocounter{section}{1}
\setcounter{theorem}{0}


\begin{theorem}[Chernoff \#1]\label{cher1}
Let $X_1, \dots, X_t$ be i.i.d. discrete random variables, with range $[0, 1]$ and mean $\mu$. Then if $X = \frac{\sum_{i=1}^t X_i}{t}$, and $0 < \delta < 1$, we have

$$
P[|X - \mu| \ge \delta \mu] \le 2 e^{\frac{-t \mu \delta^2}{3}}
$$
\end{theorem}
\medskip

\begin{theorem}[Chernoff \#2]\label{cher2}
Let $X_1, \dots, X_t$ be i.i.d. discrete random variables, with range $[0, c]$ and mean $\mu$. Then if $X = \frac{\sum_{i=1}^t X_i}{t}$, and $0 < \delta < 1$, we have

$$
P[|X - \mu| \ge \delta \mu] \le 2 e^{\frac{-t \mu \delta^2}{3c}}
$$
\end{theorem}
\medskip
We must show that \ref{cher2} follows from \ref{cher1}.

\bigskip

\begin{proof}
Let $X_1, \dots, X_t$ be i.i.d. discrete random variables, with range $[0, c]$ and mean $\mu$. Let $X = \frac{\sum_{i=1}^t X_i}{t}$, and $0 < \delta < 1$. We must show that $P\left[\left|X_i - \mu\right| \ge \delta \mu\right] \le 2 e^{\frac{-t \mu \delta^2}{3c}}$.

For $1 \le i \le t$, call $Y_i = \frac{X_i}{c}$. If the p.d.f. for $X_i$ was $f$, then the p.d.f. for $Y_i$ is $\frac{f}{c}$, thus the $Y_i$ are identically distributed. Furthermore they are independent, since if $1 \le i, j \le t$ and $0 \le y, y' \le c$, then

{\color{red}
En realidad despues mostras que el rango de Y es entre 0 y 1, por lo tanto $y$ y $y'$ no pueden ir enre 0 y c, no? Si es asi, yo pondria primero el rango de Y, y después probaría independencia (es un detalle menor que no cambia la demostración de independencia, nomás por prolijidad).
}

\begin{align*}
P[Y_i = y, Y_j = y'] &= P\left[\frac{X_i}{c} = y,  \frac{X_j}{c} = y'\right]&\\
                     &= P\left[X_i = cy, X_j = cy'\right]&\\
                     &= P\left[X_i = cy \right] \cdot P\left[X_j = cy' \right] &\text{\emph{because $X_i$ and $X_j$ are independent}}\\
                     &= P\left[\frac{X_i}{c} = y\right] \cdot P\left[\frac{X_j}{c} = y'\right]&\\
                     &= P\left[Y_i = y\right] \cdot P\left[Y_j = y' \right]&
\end{align*}

Since the range of $X_i$ is $[0, c]$, the range of $Y_i = \frac{X_i}{c}$ is $\left[\frac{0}{c}, \frac{c}{c}\right] = [0, 1]$.

Finally, let's compute $Y_i$'s expected value. For $1 \le i \le t$ we have $\mathbb{E}[Y_i] = \mathbb{E}\left[\frac{X_i}{c}\right] = \frac{\mathbb{E}\left[X_i\right]}{c} = \frac{\mu}{c}$ by linearity of expectation. 

Thus, if we make $Y = \frac{\sum_{i=1}^t Y_i}{t}$, by \ref{cher1} we know that for $1 \le i \le t$

$$
P\left[\underbrace{\left|Y - \frac{\mu}{c}\right| \ge \delta \frac{\mu}{c}}_{\theta}\right] \le 2 e^{\frac{-t \frac{\mu}{c} \delta^2}{3}}
$$

{\color{red}
Esto lo cambie: antes decia "$Pr [|Y_i - \frac{\mu}{c}|]$ etc, agregue la sumatoria de las $Y_i$. Fijense que el teorema se usa tomando el promedio de los $X_i$ ($Y_i$ en este caso), y habla sobre la distancia entre "el promedio y la media", no entre la variable y la media.
}

We also know that $Y_i = \frac{X_i}{c}$, so 

$$\left|Y - \frac{\mu}{c}\right| = 
\left|\frac{\sum_{i=1}^t \frac{X_i}{c}}{t} - \frac{\mu}{c}\right| = 
\left|\frac{\sum_{i=1}^t \frac{X_i}{t}}{c} - \frac{\mu}{c}\right| = 
\left|\frac{X}{c} - \frac{\mu}{c}\right| = \left|\frac{X - \mu}{c}\right| = \frac{\left|X - \mu\right|}{c}$$

since $c > 0$. Thus in $\theta$ we can multiply both sides of the inequality by $c$ and obtain

$$
P\left[\left|X - \mu\right| \ge \delta \mu\right] \le 2 e^{\frac{-t \frac{\mu}{c} \delta^2}{3}} = 2 e^{\frac{-t \mu \delta^2}{3c}}
$$

%The exponent on the right hand side is clearly $$, which completes the proof of \ref{cher2}.

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\section*{Problem 2}
\addtocounter{section}{1}
\setcounter{theorem}{0}

{\color{red}
Acá se olvidó de pedir que $\mu > 0$ para que valga lo segundo, lo agrego en las hipótesis, ya le mandé un mail avisando.
}

\begin{theorem}[Chebyshev \#1]\label{cheb1}
Let $X$ be a random variable with finite mean $\mu$ and variance $\sigma^2 \ne 0$. Then for every $c \in \mathbb{R}$, $c > 0$

$$
P\left[|X - \mu| \ge c \sigma\right] \le \frac{1}{c^2}
$$
\end{theorem}
\medskip

\begin{theorem}[Chebyshev \#2]\label{cheb2}
Let $X$ be a random variable with finite mean $\mu > 0$ and variance $\sigma^2 \ne 0$. Then for every $c' \in \mathbb{R}$, $c' > 0$

$$
P\left[|X - \mu| \ge c' \mu\right] \le \frac{\sigma^2}{(c'\mu)^2}
$$
\end{theorem}
\medskip

We must show that \ref{cheb2} follows from \ref{cheb1}.
\bigskip

\begin{proof}
Let $X$ be a random variable with finite mean $\mu > 0$ and variance $\sigma^2 \ne 0$;
$c' \in \mathbb{R}$ such that $c' > 0$; and let $c = \frac{c' \mu}{\sigma}$. Clearly $c \in \mathbb{R}$ and $c > 0$, since $\sigma = \sqrt{\sigma^2} > 0$, $\mu > 0$, and $c' > 0$.

We use $X$ and $c$ in \ref{cheb1} to obtain the bound

$$
P\left[|X - \mu| \ge c \sigma\right] \le \frac{1}{c^2}
$$

Note that $c \sigma = \frac{c' \mu}{\sigma} \sigma = c' \mu$. Also note that $\frac{1}{c^2} = \left(\frac{1}{\frac{c' \mu}{\sigma}}\right)^2 = \frac{\sigma^2}{(c' \mu)^2}$, thus we obtain

$$
P\left[|X - \mu| \ge c' \mu\right] \le \frac{\sigma^2}{(c' \mu)^2}
$$

Completing the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\section*{Problem 3}
\addtocounter{section}{1}
\setcounter{theorem}{0}

\begin{definition}
Let $m \in \mathbb{N}$, $f = \langle f_1, \dots, f_m \rangle \in \mathbb{N}_0^m$, $k \in \mathbb{N}, k \le m$. We define

$$
Err^k(f) = \min_{g : \norm{g}_0 \le k}{\norm{f - g}_1}
$$
\end{definition}

\begin{theorem}
Let $m \in \mathbb{N}$, $f = \langle f_1, \dots, f_m \rangle \in \mathbb{N}_0^m$, $k \in \mathbb{N}, k \le m$. Then $Err^k(f) = \sum_{i \not\in S} |f_i|$, where $S$ is some set of indices of $k$ largest values in $f$.
\end{theorem}

To prove this theorem we will use two axiliary lemmas:

\begin{lemma}\label{lemS1}
Let $m \in \mathbb{N}$, $f = \langle f_1, \dots, f_m \rangle \in \mathbb{N}_0^m$, $k \in \mathbb{N}, k \le m$. Let $g \in \mathbb{N}_0^m$ be such that $\norm{f - g}$ is minimized among the $g'$ such that $\norm{g'} \le k$.

Then $g = f_S$ for some $S \subset [m]$.
\end{lemma}
\begin{proof}
Let $T$ be the nonzero indices of $g$. Suppose, by contradiction, that for some index $i \in T$, we have $g_i \ne f_i$. In particular, $|f_i - g_i| > 0$.

Let $g' \in \mathbb{N}_0^m$ be such that $g'_j = g_j \forall j \ne i$, and $g'_i = f_i$. We have changed an entry of $g$ that is in $T$, so we are not adding any new nonzero entries to $g$ (we may at most be removing one, that is, setting an entry to zero, if $f_i = 0$). Thus 

\begin{align}\label{fit}
\norm{g} \le \norm{g'}_0 \le k
\end{align}

Specifically, every index that is outside $T$ in $g'$ has a 0 in its entry.

However, we find that

\begin{align*}
\norm{f - g}_1 &= \sum_{j = 1}^m |f_j - g_j|^1\\
               &= \sum_{j = 1}^m |f_j - g_j|\\
               &= \sum_{j \in T} |f_j - g_j| + \sum_{j \not\in T} |f_j - g_j|\\
               &= \sum_{j \in T} |f_j - g_j| + \sum_{j \not\in T} |f_j| & \text{By definition of }T\\
               &= |f_i - g_i| + \sum_{\substack{j \in T\\j \ne i}} |f_j - g_j| + \sum_{j \not\in T} |f_j|\\
               &> 0 + \sum_{\substack{j \in T\\j \ne i}} |f_j - g_j| + \sum_{j \not\in T} |f_j| & \text{Since we are dropping a positive term}\\
               &= |f_i - g'_i| + \sum_{\substack{j \in T\\j \ne i}} |f_j - g_j| + \sum_{j \not\in T} |f_j| & \text{By definition of }g'\\
               &= |f_i - g'_i| + \sum_{\substack{j \in T\\j \ne i}} |f_j - g'_j| + \sum_{j \not\in T} |f_j| & \text{By definition of }g'\\
               &= |f_i - g'_i| + \sum_{\substack{j \in T\\j \ne i}} |f_j - g'_j| + \sum_{j \not\in T} |f_j - g'_j| & \text{By definition of }g'\\
               &= \sum_{j \in T} |f_j - g'_j| + \sum_{j \not\in T} |f_j - g'_j|\\
               &= \sum_{j = 1}^m |f_j - g'_j|\\
               &= \sum_{j = 1}^m |f_j - g'_j|^1\\
               &= \norm{f - g'}_1
\end{align*}

So $\norm{f - g}_1 > \norm{f - g'}_1$, which is absurd by (\ref{fit}) and by definition of $g$ as minimizing that norm.
\end{proof}

\begin{lemma}\label{lemS2}
Let $m \in \mathbb{N}$, $f = \langle f_1, \dots, f_m \rangle \in \mathbb{N}_0^m$, $k \in \mathbb{N}, k \le m$, $S$ be some set of indices of $k$ largest values in $f$, and $S'$ any set of indices of $f$, with $S' \le k$. 

Then $\norm{f - f_S}_1 \le \norm{f - f_{S'}}_1$.
\end{lemma}

\begin{proof}
Let us denote $s = \sum_{i = 1}^m |f_i|$.
\begin{align*}
\norm{f - f_S}_1 &= \sum_{i = 1}^m |f_i - {f_S}_i|\\
                 &= \sum_{i \in S} |f_i - {f_S}_i| + \sum_{i \not\in S} |f_i - {f_S}_i|\\
                 &= \sum_{i \in S} |f_i - f_i| + \sum_{i \not\in S} |f_i - 0| & \text{By definition of }f_S\\
                 &= \sum_{i \not\in S} |f_i|\\
                 &= s - \sum_{i \in S} |f_i| 
\end{align*}

Similarly $\norm{f - f_{S'}}_1 = s - \sum_{i \in S'} |f_i|$. But then, since $S$ are $k$ largest in $f$, it must be the case that their sum is also larger than the one of any set $S'$ of smaller size. That is, $\sum_{i \in S} |f_i| \ge \sum_{i \in S'} |f_i|$. By negating and adding $s$ to each side we obtain $\norm{f - f_S} \le \norm{f - f_{S'}}$, completing the proof.
\end{proof}

We now prove the main result.
\begin{proof}
Let $m \in \mathbb{N}$, $f = \langle f_1, \dots, f_m \rangle \in \mathbb{N}_0^m$, $k \in \mathbb{N}, k \le m$, and $S$ be some set of indices of $k$ largest values in $f$.

We note by $f_S$ the vector in $\mathbb{N}_0^m$ such that

$$
{f_S}_i = \begin{cases}
f_i & \text{if } i \in S\\
0 & \text{otherwise}
\end{cases}
$$

It is clear that, since $S$ is a set of indices, and $|S| = k$, then $\norm{f_S}_0 \le k$, since at least $m - k$ entries vanish when subscripting by $S$, thus $\norm{f_S}_0 = \sum_{i=1}^m |{f_S}_i|^0 = \sum_{i \in S} |{f_S}_i|^0 \le \sum_{i \in S} 1 = |S| = k$.

We also see that

\begin{align*}
\norm{f - f_S}_1 &= \sum_{i = 1}^m |f_i - {f_S}_i|^1\\
                 &= \sum_{i = 1}^m |f_i - {f_S}_i|\\
                 &= \sum_{i \in S} |f_i - {f_S}_i| + \sum_{i \not\in S} |f_i - {f_S}_i|\\
                 &= \sum_{i \in S} |f_i - f_i| + \sum_{i \not\in S} |f_i - 0| & \text{By definition of $f_S$}\\
                 &= 0 + \sum_{i \not\in S} |f_i|
\end{align*}

Thus, since $Err^k(f)$ is the minimum over all suitable $g$s of $\norm{f - g}_1$, and $f_S$ is one suitable $g$ since $\norm{f_S}_0 \le k$, we know $Err^k(f) \le \norm{f - f_S}_1 =  \sum_{i \not\in S} |f_i|$.

We now need to prove the symmetric bound, $Err^k(f) \ge \sum_{i \not\in S} |f_i|$.

Let $g$ be any vector such that $\norm{g}_0 \le k$ and $\norm{f - g}_1$ is minimized. By definition, $Err^k(f) = \norm{f - g}_1$.


We know by \ref{lemS1} that $g$ must be of the form $g = f_{S'}$ for some $S' \subset [m]$. By \ref{lemS2} we know $\norm{f - f_{S'}}_1 \ge \norm{f - f_S}_1$. Thus

\begin{align*}
Err^k(f) &= \norm{f - g}_1\\
         &= \norm{f - f_{S'}}_1\\
         &\ge \norm{f - f_S}_1\\
         &= \sum_{i \not\in S} |f_i|
\end{align*}

This upper bound, along with the previous lower bound, shows $Err^k(f) = \sum_{i \not\in S} |f_i|$, completing the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section*{Problem 4}
\addtocounter{section}{1}
\setcounter{theorem}{0}

\begin{quote}
	Let $S = \{ x_1, ..., x_m \}$ be a stream of $m$ elements taken from universe $[n] = \{1, 2, ..., n \}$. Define $rank(y) = \left| x \in S : x \leq y \right|$. Then the median of the stream is $M = x_i$, where $rank(x_i) = \frac{m}{2} + 1$, with $m$ odd.
	Given an algorithm that returns $y = $ the median of a sample of size $t$ taken from $S$ (with replacement). If $t = O(n)$, is it true that, with probability $\geq \frac{2}{3}$, $M - \frac{n}{10} < y < M + \frac{n}{10}$? (i.e, does this algorithm give a $10\%$ approximate value of the median with probability $\geq \frac{2}{3}$?).
\end{quote}

\bigskip
The anser is no. Let's see why.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Problem 5}
\addtocounter{section}{1}
\setcounter{theorem}{0}

\begin{theorem}Let $x = \langle x_1, \dots, x_m \rangle$, $f = \langle f_1, \dots, f_n \rangle$, where $f_i$ is the frequency of the $i$th distinct item in the list $x$. Let $F_k = \sum_{i=1}^n f_i^k$.

Then $F_k \ge n \left(\frac{m}{n}\right)^k$.
\end{theorem}

We require a few auxiliary results to prove this.

\begin{theorem}[Hölder inequality for $\mathbb{R}^n$] Let $p, q \in [1, \infty]$ such that $\frac{1}{p} + \frac{1}{q} = 1$. Let $n \in \mathbb{N}$, $a, b \in \mathbb{R}^n$. Then

$$
\sum_{i=1}^n |a_i b_i| \le \left(\sum_{i=1}^n |a_i|^p\right)^{\frac{1}{p}} + \left(\sum_{i=1}^n |b_i|^q\right)^{\frac{1}{q}}
$$
\end{theorem}

\begin{corollary}\label{cor1}
Let $r \in [1, \infty]$, $n \in \mathbb{N}$, $a, b \in \mathbb{R}^n$. Then

$$
\sum_{i=1}^n |a_i b_i| \le \left(\sum_{i=1}^n |a_i|^r\right)^{\frac{1}{r}} + \left(\sum_{i=1}^n |b_i|^{\frac{r}{r-1}}\right)^{1 - \frac{1}{r}}
$$
\end{corollary}

\begin{lemma}\label{lem1}
Let $n \in \mathbb{N}$, $x \in \mathbb{R}^n$, and $p, q \in \mathbb{R}$ such that $0 < p < q$. Then $\norm{x}_p \le \norm{x}_q n^{\frac{1}{p} - \frac{1}{q}}$.
\end{lemma}
\begin{proof}
Use \ref{cor1} with $a_i = |x_i|^p, b_i = 1, r = \frac{q}{p}$ to obtain

\begin{align*}
\sum_{i = 1}^n |x_i|^p &= \sum_{i = 1}^n |x_i|^p \cdot 1\\
                       &\le \left(\sum_{i=1}^n \left(|x_i|^p\right)^{\frac{q}{p}}\right)^{\frac{p}{q}} \left(\sum_{i=1}^n 1^{\frac{q}{q-p}}\right)^{1 - \frac{p}{q}}\\
                       &= \left(\sum_{i=1}^n \left(|x_i|^p\right)^{\frac{q}{p}}\right)^{\frac{p}{q}} n^{1 - \frac{p}{q}}
\end{align*}

Then

\begin{align*}
\norm{x}_p &= \left(\sum_{i=1}^n |x_i|^p \right)^{\frac{1}{p}}\\
           &\le \left(\left(\sum_{i=1}^n \left(|x_i|^p\right)^{\frac{q}{p}}\right)^{\frac{p}{q}} n^{1 - \frac{p}{q}}\right)^{\frac{1}{p}}\\
           &= \left(\sum_{i=1}^n \left(|x_i|^p\right)^{\frac{q}{p}}\right)^{\frac{1}{q}} n^{\frac{1}{p} - \frac{1}{q}}\\
           &= \norm{x}_q n^{\frac{1}{p} - \frac{1}{q}}
\end{align*}
\end{proof}

\begin{corollary}\label{cor2}
Let $n \in \mathbb{N}$, $x \in \mathbb{R}^n$, and $p, q \in \mathbb{R}$ such that $0 < p < q$. Then $\norm{x}_q \ge \norm{x}_p n^{\frac{1}{q} - \frac{1}{p}}$.
\end{corollary}

With that scaffolding done, we prove the main theorem.

\begin{proof}
We must show that $F_k \ge n \left(\frac{m}{n}\right)^k$.

We note that $m = \sum_{i=1}^n f_i = \sum_{i=1}^n |f_i| = \norm{f}_1$. We also note that $F_k = \sum_{i=1}^n f_i^k = \sum_{i=1}^n |f_i|^k = \left(\norm{f}_k\right)^k$.

Thus, for $k = 1$ the result is trivial.

For $k > 1$, since $k \in \mathbb{N}$, we have $k \ge 2$. We also have

\begin{align*}
\sqrt[k]{n} \frac{m}{n} &= \frac{\sqrt[k]{n}}{n} m\\
                        &= \frac{\sqrt[k]{n}}{\left(\sqrt[k]{n}\right)^k} \norm{f}_1\\
                        &= \frac{1}{\left(\sqrt[k]{n}\right)^{k-1}} \norm{f}_1\\
                        &= n^{-\frac{k-1}{k}} \norm{f}_1\\
                        &= n^{\frac{1}{k} - 1} \norm{f}_1
\end{align*}

Thus by \ref{cor2} we call $p = 1$, $q = k$ and since $0 < p < q$ we obtain

\begin{align*}
\norm{f}_k &\ge n^{\frac{1}{k} - 1} \norm{f}_1\\
           &= \sqrt[k]{n} \frac{m}{n}
\end{align*}

Taking $k$th powers of both sides completes the proof.
\end{proof}

\end{document}  